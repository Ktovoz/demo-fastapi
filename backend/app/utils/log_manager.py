"""
æ—¥å¿—ç®¡ç†å·¥å…·
ç”¨äºç®¡ç†åº”ç”¨ç¨‹åºæ—¥å¿—æ–‡ä»¶
"""

import os
import gzip
import shutil
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Any
from app.core.config import settings
from app.utils.logger import get_logger

logger = get_logger(__name__)


class LogManager:
    """æ—¥å¿—ç®¡ç†å™¨"""

    def __init__(self, log_dir: str = None):
        self.log_dir = Path(log_dir or "logs")
        self.log_dir.mkdir(exist_ok=True)

    def get_log_files(self) -> Dict[str, List[Dict[str, Any]]]:
        """è·å–æ‰€æœ‰æ—¥å¿—æ–‡ä»¶ä¿¡æ¯"""
        log_files = {
            "current": [],  # å½“å‰ä½¿ç”¨çš„æ—¥å¿—æ–‡ä»¶
            "archived": [], # å·²å½’æ¡£çš„æ—¥å¿—æ–‡ä»¶
            "compressed": [] # å‹ç¼©çš„æ—¥å¿—æ–‡ä»¶
        }

        for file_path in self.log_dir.rglob("*"):
            if file_path.is_file():
                stat = file_path.stat()
                file_info = {
                    "path": str(file_path),
                    "name": file_path.name,
                    "size_mb": round(stat.st_size / (1024 * 1024), 2),
                    "modified": datetime.fromtimestamp(stat.st_mtime),
                    "size": stat.st_size
                }

                if file_path.suffix == '.gz':
                    log_files["compressed"].append(file_info)
                elif file_path.name.endswith('.log'):
                    log_files["current"].append(file_info)
                else:
                    log_files["archived"].append(file_info)

        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
        for category in log_files:
            log_files[category].sort(key=lambda x: x["modified"], reverse=True)

        return log_files

    def cleanup_old_logs(self, days: int = None) -> Dict[str, int]:
        """æ¸…ç†è¶…è¿‡æŒ‡å®šå¤©æ•°çš„æ—§æ—¥å¿—æ–‡ä»¶"""
        retention_days = days or int(settings.LOG_RETENTION.split()[0]) if settings.LOG_RETENTION.isdigit() else 7
        cutoff_date = datetime.now() - timedelta(days=retention_days)

        deleted_count = 0
        deleted_size = 0

        for file_path in self.log_dir.rglob("*"):
            if file_path.is_file():
                stat = file_path.stat()
                file_date = datetime.fromtimestamp(stat.st_mtime)

                if file_date < cutoff_date:
                    try:
                        file_size = file_path.stat().st_size
                        file_path.unlink()
                        deleted_count += 1
                        deleted_size += file_size
                        logger.info(f"åˆ é™¤æ—§æ—¥å¿—æ–‡ä»¶: {file_path.name}")
                    except Exception as e:
                        logger.error(f"åˆ é™¤æ—¥å¿—æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

        return {
            "deleted_files": deleted_count,
            "freed_size_mb": round(deleted_size / (1024 * 1024), 2)
        }

    def compress_logs(self, max_age_days: int = 1) -> Dict[str, int]:
        """å‹ç¼©è¶…è¿‡æŒ‡å®šå¤©æ•°çš„æœªå‹ç¼©æ—¥å¿—æ–‡ä»¶"""
        cutoff_date = datetime.now() - timedelta(days=max_age_days)
        compressed_count = 0
        compressed_size = 0

        for file_path in self.log_dir.glob("*.log"):
            if file_path.is_file() and not file_path.name.endswith('.gz'):
                stat = file_path.stat()
                file_date = datetime.fromtimestamp(stat.st_mtime)

                if file_date < cutoff_date:
                    try:
                        # å‹ç¼©æ–‡ä»¶
                        compressed_path = file_path.with_suffix(file_path.suffix + '.gz')
                        with open(file_path, 'rb') as f_in:
                            with gzip.open(compressed_path, 'wb') as f_out:
                                shutil.copyfileobj(f_in, f_out)

                        # è·å–æ–‡ä»¶å¤§å°ä¿¡æ¯
                        original_size = file_path.stat().st_size
                        compressed_size_stat = compressed_path.stat().st_size
                        compressed_size += original_size
                        compressed_count += 1

                        # åˆ é™¤åŸæ–‡ä»¶
                        file_path.unlink()

                        compression_ratio = (1 - compressed_size_stat / original_size) * 100
                        logger.info(f"å‹ç¼©æ—¥å¿—æ–‡ä»¶: {file_path.name} (å‹ç¼©ç‡: {compression_ratio:.1f}%)")

                    except Exception as e:
                        logger.error(f"å‹ç¼©æ—¥å¿—æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

        return {
            "compressed_files": compressed_count,
            "original_size_mb": round(compressed_size / (1024 * 1024), 2)
        }

    def get_log_stats(self) -> Dict[str, Any]:
        """è·å–æ—¥å¿—ç»Ÿè®¡ä¿¡æ¯"""
        log_files = self.get_log_files()

        total_size = 0
        total_files = 0

        for category, files in log_files.items():
            for file_info in files:
                total_size += file_info["size"]
                total_files += 1

        # æŒ‰ç±»å‹ç»Ÿè®¡
        size_by_type = {}
        for category, files in log_files.items():
            size_by_type[category] = {
                "count": len(files),
                "size_mb": round(sum(f["size"] for f in files) / (1024 * 1024), 2)
            }

        return {
            "total_files": total_files,
            "total_size_mb": round(total_size / (1024 * 1024), 2),
            "log_directory": str(self.log_dir),
            "by_type": size_by_type,
            "details": log_files
        }

    def rotate_logs_manually(self, max_size_mb: int = 10):
        """æ‰‹åŠ¨è½®è½¬è¶…è¿‡æŒ‡å®šå¤§å°çš„æ—¥å¿—æ–‡ä»¶"""
        rotated_count = 0

        for file_path in self.log_dir.glob("*.log"):
            if file_path.is_file():
                size_mb = file_path.stat().st_size / (1024 * 1024)
                if size_mb > max_size_mb:
                    try:
                        # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å¤‡ä»½æ–‡ä»¶
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        backup_path = file_path.with_name(
                            f"{file_path.stem}_{timestamp}{file_path.suffix}"
                        )
                        file_path.rename(backup_path)
                        rotated_count += 1
                        logger.info(f"æ‰‹åŠ¨è½®è½¬æ—¥å¿—æ–‡ä»¶: {file_path.name} -> {backup_path.name}")
                    except Exception as e:
                        logger.error(f"è½®è½¬æ—¥å¿—æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

        return {"rotated_files": rotated_count}


def get_log_manager() -> LogManager:
    """è·å–æ—¥å¿—ç®¡ç†å™¨å®ä¾‹"""
    return LogManager()


if __name__ == "__main__":
    # æµ‹è¯•æ—¥å¿—ç®¡ç†åŠŸèƒ½
    manager = get_log_manager()

    # æ˜¾ç¤ºæ—¥å¿—ç»Ÿè®¡
    stats = manager.get_log_stats()
    print("ğŸ“Š æ—¥å¿—ç»Ÿè®¡ä¿¡æ¯:")
    print(f"æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
    print(f"æ€»å¤§å°: {stats['total_size_mb']} MB")
    print(f"æ—¥å¿—ç›®å½•: {stats['log_directory']}")

    for category, info in stats['by_type'].items():
        print(f"{category}: {info['count']} ä¸ªæ–‡ä»¶, {info['size_mb']} MB")